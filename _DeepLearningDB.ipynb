{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02ff8b82-771c-476c-abe3-e6f0de81d29a",
   "metadata": {},
   "source": [
    "## Codigo Base: Problema de regresión simple con un conjunto de datos generado sintéticamente, se usa una red neuronal simple para predecir un valor continuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c01fd37-c4ee-4489-9185-856478295ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 13.281258583068848, Validation Loss: 12.506816864013672\n",
      "Epoch 2/20, Training Loss: 13.12148666381836, Validation Loss: 12.350800514221191\n",
      "Epoch 3/20, Training Loss: 12.962525367736816, Validation Loss: 12.194937705993652\n",
      "Epoch 4/20, Training Loss: 12.803288459777832, Validation Loss: 12.040984153747559\n",
      "Epoch 5/20, Training Loss: 12.645622253417969, Validation Loss: 11.88829231262207\n",
      "Epoch 6/20, Training Loss: 12.488896369934082, Validation Loss: 11.7362060546875\n",
      "Epoch 7/20, Training Loss: 12.332866668701172, Validation Loss: 11.584333419799805\n",
      "Epoch 8/20, Training Loss: 12.177026748657227, Validation Loss: 11.435091972351074\n",
      "Epoch 9/20, Training Loss: 12.023601531982422, Validation Loss: 11.286783218383789\n",
      "Epoch 10/20, Training Loss: 11.870782852172852, Validation Loss: 11.139001846313477\n",
      "Epoch 11/20, Training Loss: 11.717995643615723, Validation Loss: 10.991019248962402\n",
      "Epoch 12/20, Training Loss: 11.564897537231445, Validation Loss: 10.842602729797363\n",
      "Epoch 13/20, Training Loss: 11.411850929260254, Validation Loss: 10.693218231201172\n",
      "Epoch 14/20, Training Loss: 11.257671356201172, Validation Loss: 10.542816162109375\n",
      "Epoch 15/20, Training Loss: 11.102311134338379, Validation Loss: 10.392082214355469\n",
      "Epoch 16/20, Training Loss: 10.946366310119629, Validation Loss: 10.24170207977295\n",
      "Epoch 17/20, Training Loss: 10.79031753540039, Validation Loss: 10.091431617736816\n",
      "Epoch 18/20, Training Loss: 10.634203910827637, Validation Loss: 9.941852569580078\n",
      "Epoch 19/20, Training Loss: 10.479294776916504, Validation Loss: 9.792557716369629\n",
      "Epoch 20/20, Training Loss: 10.324344635009766, Validation Loss: 9.643088340759277\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generar datos sintéticos\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 1)\n",
    "y = 3 * X.squeeze() + 2 + np.random.randn(1000) * 0.1\n",
    "\n",
    "# Preparar los datos\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Definir el modelo\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Función para entrenar el modelo\n",
    "def train_model(model, X_train, y_train, X_val, y_val, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "        \n",
    "# Entrenar con tasa de aprendizaje por defecto\n",
    "train_model(model, X_train, y_train, X_val, y_val, criterion, optimizer, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0316c46-4a86-48af-bc05-38cccda23770",
   "metadata": {},
   "source": [
    "## **Desafío 1: Cambiar la Tasa de Aprendizaje del Optimizador Adam**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1729922-e643-4050-b510-0fc8d0d0c99e",
   "metadata": {},
   "source": [
    "### 1. Cambiar la tasa de aprendizaje a 0.01 y 0.0001:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb8db784-d2e9-41eb-99f7-0d2d447ac7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 10.168959617614746, Validation Loss: 8.176473617553711\n",
      "Epoch 2/20, Training Loss: 8.639582633972168, Validation Loss: 6.705957412719727\n",
      "Epoch 3/20, Training Loss: 7.100825786590576, Validation Loss: 5.309114933013916\n",
      "Epoch 4/20, Training Loss: 5.6183857917785645, Validation Loss: 4.05067253112793\n",
      "Epoch 5/20, Training Loss: 4.254598617553711, Validation Loss: 3.0119736194610596\n",
      "Epoch 6/20, Training Loss: 3.0931949615478516, Validation Loss: 2.280740976333618\n",
      "Epoch 7/20, Training Loss: 2.23103404045105, Validation Loss: 1.9084994792938232\n",
      "Epoch 8/20, Training Loss: 1.746081829071045, Validation Loss: 1.7939914464950562\n",
      "Epoch 9/20, Training Loss: 1.5635372400283813, Validation Loss: 1.7739348411560059\n",
      "Epoch 10/20, Training Loss: 1.5394896268844604, Validation Loss: 1.7103811502456665\n",
      "Epoch 11/20, Training Loss: 1.5265923738479614, Validation Loss: 1.5613964796066284\n",
      "Epoch 12/20, Training Loss: 1.4522746801376343, Validation Loss: 1.3104404211044312\n",
      "Epoch 13/20, Training Loss: 1.2732911109924316, Validation Loss: 1.011061668395996\n",
      "Epoch 14/20, Training Loss: 1.023552417755127, Validation Loss: 0.7334483861923218\n",
      "Epoch 15/20, Training Loss: 0.7712235450744629, Validation Loss: 0.5367686748504639\n",
      "Epoch 16/20, Training Loss: 0.5793927907943726, Validation Loss: 0.44683876633644104\n",
      "Epoch 17/20, Training Loss: 0.481550931930542, Validation Loss: 0.4554521143436432\n",
      "Epoch 18/20, Training Loss: 0.47629761695861816, Validation Loss: 0.5259203314781189\n",
      "Epoch 19/20, Training Loss: 0.5331801176071167, Validation Loss: 0.6164243817329407\n",
      "Epoch 20/20, Training Loss: 0.6131743788719177, Validation Loss: 0.6923670172691345\n"
     ]
    }
   ],
   "source": [
    "# Tasa de aprendizaje = 0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "train_model(model, X_train, y_train, X_val, y_val, criterion, optimizer, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "feb8690d-36a1-4394-9338-dee3c1761650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 0.6824318766593933, Validation Loss: 0.6865000128746033\n",
      "Epoch 2/20, Training Loss: 0.6769404411315918, Validation Loss: 0.6806812286376953\n",
      "Epoch 3/20, Training Loss: 0.6714969873428345, Validation Loss: 0.6749125123023987\n",
      "Epoch 4/20, Training Loss: 0.666103184223175, Validation Loss: 0.6691948175430298\n",
      "Epoch 5/20, Training Loss: 0.6607597470283508, Validation Loss: 0.6635289788246155\n",
      "Epoch 6/20, Training Loss: 0.655465841293335, Validation Loss: 0.6579152941703796\n",
      "Epoch 7/20, Training Loss: 0.6502227187156677, Validation Loss: 0.6523557305335999\n",
      "Epoch 8/20, Training Loss: 0.6450315713882446, Validation Loss: 0.6468483209609985\n",
      "Epoch 9/20, Training Loss: 0.6398932933807373, Validation Loss: 0.6413967609405518\n",
      "Epoch 10/20, Training Loss: 0.6348080635070801, Validation Loss: 0.6360017657279968\n",
      "Epoch 11/20, Training Loss: 0.629776120185852, Validation Loss: 0.6306604146957397\n",
      "Epoch 12/20, Training Loss: 0.624798059463501, Validation Loss: 0.6253761053085327\n",
      "Epoch 13/20, Training Loss: 0.6198732852935791, Validation Loss: 0.6201468110084534\n",
      "Epoch 14/20, Training Loss: 0.6150002479553223, Validation Loss: 0.6149731874465942\n",
      "Epoch 15/20, Training Loss: 0.6101793050765991, Validation Loss: 0.6098560094833374\n",
      "Epoch 16/20, Training Loss: 0.6054105162620544, Validation Loss: 0.604795515537262\n",
      "Epoch 17/20, Training Loss: 0.6006929278373718, Validation Loss: 0.5997915863990784\n",
      "Epoch 18/20, Training Loss: 0.5960257053375244, Validation Loss: 0.5948421955108643\n",
      "Epoch 19/20, Training Loss: 0.5914079546928406, Validation Loss: 0.5899487137794495\n",
      "Epoch 20/20, Training Loss: 0.586837887763977, Validation Loss: 0.5851112604141235\n"
     ]
    }
   ],
   "source": [
    "# Tasa de aprendizaje = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "train_model(model, X_train, y_train, X_val, y_val, criterion, optimizer, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33412628-e9e3-4d58-9952-fcfed13ba894",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Pregunta: ¿Cómo afecta cada cambio en la tasa de aprendizaje a la precisión y la pérdida del modelo durante el entrenamiento y la validación?\n",
    "\n",
    "#### Tasa de Aprendizaje 0.01: Proporciona una convergencia rápida, pero podría resultar en inestabilidad si se aplicara a problemas más complejos.\n",
    "#### Tasa de Aprendizaje 0.0001: Converge de manera muy estable, pero es muy lenta y no siempre práctica para problemas grandes o urgentes.\n",
    "\n",
    "#### Para problemas sencillos, una tasa de aprendizaje más alta (0.01) puede ser útil para una convergencia rápida.\n",
    "#### Para problemas más complejos, la tasa de aprendizaje por defecto (0.001) es una buena elección inicial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc10de31-7357-4427-802a-30ad0fd21260",
   "metadata": {},
   "source": [
    "## **Desafío 2: Modificar el Número de Neuronas en las Capas Ocultas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe1d0c2-52f0-43da-9d9d-80f71170640c",
   "metadata": {},
   "source": [
    "### 1. Cambiar el número de neuronas a 64 y 128:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5aa477b-f84e-4bea-895e-3d6bd2432282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 13.780046463012695, Validation Loss: 12.783044815063477\n",
      "Epoch 2/20, Training Loss: 13.341474533081055, Validation Loss: 12.3743896484375\n",
      "Epoch 3/20, Training Loss: 12.915091514587402, Validation Loss: 11.974186897277832\n",
      "Epoch 4/20, Training Loss: 12.496899604797363, Validation Loss: 11.578474998474121\n",
      "Epoch 5/20, Training Loss: 12.082746505737305, Validation Loss: 11.1838960647583\n",
      "Epoch 6/20, Training Loss: 11.670158386230469, Validation Loss: 10.790541648864746\n",
      "Epoch 7/20, Training Loss: 11.259299278259277, Validation Loss: 10.398346900939941\n",
      "Epoch 8/20, Training Loss: 10.848775863647461, Validation Loss: 10.007784843444824\n",
      "Epoch 9/20, Training Loss: 10.438671112060547, Validation Loss: 9.623917579650879\n",
      "Epoch 10/20, Training Loss: 10.035781860351562, Validation Loss: 9.245840072631836\n",
      "Epoch 11/20, Training Loss: 9.639259338378906, Validation Loss: 8.873337745666504\n",
      "Epoch 12/20, Training Loss: 9.247933387756348, Validation Loss: 8.506653785705566\n",
      "Epoch 13/20, Training Loss: 8.862353324890137, Validation Loss: 8.144515037536621\n",
      "Epoch 14/20, Training Loss: 8.48173713684082, Validation Loss: 7.786993026733398\n",
      "Epoch 15/20, Training Loss: 8.105838775634766, Validation Loss: 7.433685302734375\n",
      "Epoch 16/20, Training Loss: 7.734401226043701, Validation Loss: 7.085174083709717\n",
      "Epoch 17/20, Training Loss: 7.367474555969238, Validation Loss: 6.742086887359619\n",
      "Epoch 18/20, Training Loss: 7.005717754364014, Validation Loss: 6.402923107147217\n",
      "Epoch 19/20, Training Loss: 6.647629261016846, Validation Loss: 6.068463325500488\n",
      "Epoch 20/20, Training Loss: 6.294054985046387, Validation Loss: 5.734617710113525\n"
     ]
    }
   ],
   "source": [
    "#Modelo con 64 neuronas por capa oculta:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generar datos sintéticos\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 1)\n",
    "y = 3 * X.squeeze() + 2 + np.random.randn(1000) * 0.1\n",
    "\n",
    "# Preparar los datos\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Definir el modelo\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Función para entrenar el modelo\n",
    "def train_model(model, X_train, y_train, X_val, y_val, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "        \n",
    "# Entrenar con tasa de aprendizaje por defecto\n",
    "train_model(model, X_train, y_train, X_val, y_val, criterion, optimizer, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e9660660-1033-4a22-b923-45957974fbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 12.306906700134277, Validation Loss: 10.952959060668945\n",
      "Epoch 2/20, Training Loss: 11.395984649658203, Validation Loss: 10.128756523132324\n",
      "Epoch 3/20, Training Loss: 10.534528732299805, Validation Loss: 9.35758113861084\n",
      "Epoch 4/20, Training Loss: 9.731694221496582, Validation Loss: 8.631052017211914\n",
      "Epoch 5/20, Training Loss: 8.975798606872559, Validation Loss: 7.960684776306152\n",
      "Epoch 6/20, Training Loss: 8.27917194366455, Validation Loss: 7.321876049041748\n",
      "Epoch 7/20, Training Loss: 7.6143927574157715, Validation Loss: 6.705114841461182\n",
      "Epoch 8/20, Training Loss: 6.972183704376221, Validation Loss: 6.104701995849609\n",
      "Epoch 9/20, Training Loss: 6.3462629318237305, Validation Loss: 5.521310806274414\n",
      "Epoch 10/20, Training Loss: 5.736271858215332, Validation Loss: 4.957616806030273\n",
      "Epoch 11/20, Training Loss: 5.145145416259766, Validation Loss: 4.416508197784424\n",
      "Epoch 12/20, Training Loss: 4.5766921043396, Validation Loss: 3.8990345001220703\n",
      "Epoch 13/20, Training Loss: 4.030908107757568, Validation Loss: 3.4114906787872314\n",
      "Epoch 14/20, Training Loss: 3.5152366161346436, Validation Loss: 2.9560134410858154\n",
      "Epoch 15/20, Training Loss: 3.032170057296753, Validation Loss: 2.5340116024017334\n",
      "Epoch 16/20, Training Loss: 2.583677053451538, Validation Loss: 2.146341562271118\n",
      "Epoch 17/20, Training Loss: 2.169525384902954, Validation Loss: 1.799705982208252\n",
      "Epoch 18/20, Training Loss: 1.7977492809295654, Validation Loss: 1.5003297328948975\n",
      "Epoch 19/20, Training Loss: 1.475331425666809, Validation Loss: 1.2504570484161377\n",
      "Epoch 20/20, Training Loss: 1.2051140069961548, Validation Loss: 1.0509527921676636\n"
     ]
    }
   ],
   "source": [
    "# Modelo con 128 neuronas por capa oculta:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generar datos sintéticos\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 1)\n",
    "y = 3 * X.squeeze() + 2 + np.random.randn(1000) * 0.1\n",
    "\n",
    "# Preparar los datos\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Función para entrenar el modelo\n",
    "def train_model(model, X_train, y_train, X_val, y_val, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "        \n",
    "# Entrenar con tasa de aprendizaje por defecto\n",
    "train_model(model, X_train, y_train, X_val, y_val, criterion, optimizer, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad27dd6-1653-492a-a1d3-a3e8d357075c",
   "metadata": {},
   "source": [
    "## Pregunta: ¿Cómo cambia la precisión y la pérdida del modelo con diferentes números de neuronas en las capas ocultas?\n",
    "\n",
    "#### 64 Neuronas por Capa Oculta: El modelo muestra una mejora gradual pero la pérdida final es relativamente alta, lo que sugiere un ajuste menos preciso.\n",
    "#### 128 Neuronas por Capa Oculta: La red converge más rápidamente y alcanza una pérdida final considerablemente más baja, lo que indica un mejor ajuste y precisión.\n",
    "\n",
    "#### Aumentar el número de neuronas en las capas ocultas puede mejorar la capacidad del modelo para ajustarse a los datos, reduciendo la pérdida y aumentando la precisión, pero debe equilibrarse para evitar el sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1adb0b-76cd-43c1-a902-5ccd9f25bd46",
   "metadata": {},
   "source": [
    "## Desafío 3: Cambiar la Cantidad de Épocas de Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72593846-e284-48a9-8153-99917fa4ff06",
   "metadata": {},
   "source": [
    "### 1. Cambiar el número de épocas a 10 y 30:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1de7f7ed-9d45-4058-9210-eeb41a873bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 0.988553524017334, Validation Loss: 0.9009871482849121\n",
      "Epoch 2/10, Training Loss: 0.8253019452095032, Validation Loss: 0.797716498374939\n",
      "Epoch 3/10, Training Loss: 0.7129170894622803, Validation Loss: 0.7363119721412659\n",
      "Epoch 4/10, Training Loss: 0.6468077301979065, Validation Loss: 0.7099908590316772\n",
      "Epoch 5/10, Training Loss: 0.6202055215835571, Validation Loss: 0.7103387713432312\n",
      "Epoch 6/10, Training Loss: 0.6244121789932251, Validation Loss: 0.7278811931610107\n",
      "Epoch 7/10, Training Loss: 0.6494054198265076, Validation Loss: 0.7530186176300049\n",
      "Epoch 8/10, Training Loss: 0.6847779154777527, Validation Loss: 0.7769827842712402\n",
      "Epoch 9/10, Training Loss: 0.720786452293396, Validation Loss: 0.7927092909812927\n",
      "Epoch 10/10, Training Loss: 0.7493468523025513, Validation Loss: 0.7954270839691162\n"
     ]
    }
   ],
   "source": [
    "# 10 épocas\n",
    "train_model(model, X_train, y_train, X_val, y_val, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e5f6050f-78a4-4220-b918-d4fa9b6089df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Training Loss: 0.7647519707679749, Validation Loss: 0.7828837633132935\n",
      "Epoch 2/30, Training Loss: 0.7639759182929993, Validation Loss: 0.7551795244216919\n",
      "Epoch 3/30, Training Loss: 0.7465553283691406, Validation Loss: 0.7144273519515991\n",
      "Epoch 4/30, Training Loss: 0.7142065167427063, Validation Loss: 0.6642200350761414\n",
      "Epoch 5/30, Training Loss: 0.6703224778175354, Validation Loss: 0.6082278490066528\n",
      "Epoch 6/30, Training Loss: 0.6187491416931152, Validation Loss: 0.550355076789856\n",
      "Epoch 7/30, Training Loss: 0.5635879039764404, Validation Loss: 0.49420884251594543\n",
      "Epoch 8/30, Training Loss: 0.5086932182312012, Validation Loss: 0.4427376687526703\n",
      "Epoch 9/30, Training Loss: 0.4572981894016266, Validation Loss: 0.3980453610420227\n",
      "Epoch 10/30, Training Loss: 0.411795973777771, Validation Loss: 0.36133402585983276\n",
      "Epoch 11/30, Training Loss: 0.3736659586429596, Validation Loss: 0.3331439197063446\n",
      "Epoch 12/30, Training Loss: 0.34360766410827637, Validation Loss: 0.3130187690258026\n",
      "Epoch 13/30, Training Loss: 0.3214491903781891, Validation Loss: 0.2998387813568115\n",
      "Epoch 14/30, Training Loss: 0.30625277757644653, Validation Loss: 0.29237282276153564\n",
      "Epoch 15/30, Training Loss: 0.2968360185623169, Validation Loss: 0.2890322208404541\n",
      "Epoch 16/30, Training Loss: 0.29174941778182983, Validation Loss: 0.28826239705085754\n",
      "Epoch 17/30, Training Loss: 0.28949201107025146, Validation Loss: 0.28866568207740784\n",
      "Epoch 18/30, Training Loss: 0.2886739671230316, Validation Loss: 0.28902995586395264\n",
      "Epoch 19/30, Training Loss: 0.2880774736404419, Validation Loss: 0.28839626908302307\n",
      "Epoch 20/30, Training Loss: 0.28673020005226135, Validation Loss: 0.2860921025276184\n",
      "Epoch 21/30, Training Loss: 0.28393661975860596, Validation Loss: 0.28172826766967773\n",
      "Epoch 22/30, Training Loss: 0.27927953004837036, Validation Loss: 0.2751764953136444\n",
      "Epoch 23/30, Training Loss: 0.27260273694992065, Validation Loss: 0.2665333151817322\n",
      "Epoch 24/30, Training Loss: 0.2639789879322052, Validation Loss: 0.25607967376708984\n",
      "Epoch 25/30, Training Loss: 0.2536616027355194, Validation Loss: 0.24422645568847656\n",
      "Epoch 26/30, Training Loss: 0.24203689396381378, Validation Loss: 0.23146513104438782\n",
      "Epoch 27/30, Training Loss: 0.22957666218280792, Validation Loss: 0.21830803155899048\n",
      "Epoch 28/30, Training Loss: 0.2167806774377823, Validation Loss: 0.2052590250968933\n",
      "Epoch 29/30, Training Loss: 0.20413099229335785, Validation Loss: 0.19276121258735657\n",
      "Epoch 30/30, Training Loss: 0.19205577671527863, Validation Loss: 0.18117156624794006\n"
     ]
    }
   ],
   "source": [
    "# 30 épocas\n",
    "train_model(model, X_train, y_train, X_val, y_val, criterion, optimizer, num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181176da-5637-4c28-aca9-40a8d594474f",
   "metadata": {},
   "source": [
    "## Pregunta: ¿Cómo afecta el número de épocas a la precisión y la pérdida del modelo durante el entrenamiento y la validación? ¿Observa algún signo de sobreajuste o subajuste?\n",
    "\n",
    "#### 10 Épocas de Entrenamiento: El modelo muestra una mejora significativa en las primeras épocas, pero luego se estabiliza. La pérdida en el conjunto de validación es relativamente constante después de las primeras épocas, lo que indica que el modelo podría beneficiarse de más épocas para mejorar la precisión.\n",
    "#### 30 Épocas de Entrenamiento: Con un mayor número de épocas, el modelo sigue mejorando la precisión y reduciendo la pérdida tanto en el conjunto de entrenamiento como en el de validación. Esto sugiere que el modelo podría seguir beneficiándose de más épocas sin mostrar signos claros de sobreajuste hasta este punto.\n",
    "\n",
    "#### Añadir más épocas de entrenamiento puede ayudar a mejorar la precisión del modelo, pero es importante monitorear la pérdida en el conjunto de validación para evitar el sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21383a-e7f1-4bae-9006-71bbd7ebb352",
   "metadata": {},
   "source": [
    "## Desafío 4: Modificar la Función de Activación en las Capas Ocultas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160b38ba-5d25-4866-a2f0-0b9aa1acdf88",
   "metadata": {},
   "source": [
    "### 1. Cambiar la función de activación a sigmoid y tanh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "901f905c-b8b8-45a1-b137-4eef1b513d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 11.684296607971191, Validation Loss: 10.969398498535156\n",
      "Epoch 2/20, Training Loss: 11.486849784851074, Validation Loss: 10.778372764587402\n",
      "Epoch 3/20, Training Loss: 11.291120529174805, Validation Loss: 10.58911418914795\n",
      "Epoch 4/20, Training Loss: 11.097159385681152, Validation Loss: 10.40165901184082\n",
      "Epoch 5/20, Training Loss: 10.90500259399414, Validation Loss: 10.216045379638672\n",
      "Epoch 6/20, Training Loss: 10.71468734741211, Validation Loss: 10.03230094909668\n",
      "Epoch 7/20, Training Loss: 10.52624225616455, Validation Loss: 9.850449562072754\n",
      "Epoch 8/20, Training Loss: 10.339694023132324, Validation Loss: 9.670507431030273\n",
      "Epoch 9/20, Training Loss: 10.155057907104492, Validation Loss: 9.492488861083984\n",
      "Epoch 10/20, Training Loss: 9.9723482131958, Validation Loss: 9.316400527954102\n",
      "Epoch 11/20, Training Loss: 9.791572570800781, Validation Loss: 9.142255783081055\n",
      "Epoch 12/20, Training Loss: 9.612744331359863, Validation Loss: 8.970067024230957\n",
      "Epoch 13/20, Training Loss: 9.435876846313477, Validation Loss: 8.799848556518555\n",
      "Epoch 14/20, Training Loss: 9.26098346710205, Validation Loss: 8.631616592407227\n",
      "Epoch 15/20, Training Loss: 9.088083267211914, Validation Loss: 8.46538257598877\n",
      "Epoch 16/20, Training Loss: 8.917187690734863, Validation Loss: 8.301164627075195\n",
      "Epoch 17/20, Training Loss: 8.748312950134277, Validation Loss: 8.138973236083984\n",
      "Epoch 18/20, Training Loss: 8.581473350524902, Validation Loss: 7.97882080078125\n",
      "Epoch 19/20, Training Loss: 8.416680335998535, Validation Loss: 7.820717811584473\n",
      "Epoch 20/20, Training Loss: 8.253944396972656, Validation Loss: 7.664671421051025\n"
     ]
    }
   ],
   "source": [
    "#Modelo con función de activación sigmoid:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generar datos sintéticos\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 1)\n",
    "y = 3 * X.squeeze() + 2 + np.random.randn(1000) * 0.1\n",
    "\n",
    "# Preparar los datos\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Definir el modelo\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Función para entrenar el modelo\n",
    "def train_model(model, X_train, y_train, X_val, y_val, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "        \n",
    "# Entrenar con tasa de aprendizaje por defecto\n",
    "train_model(model, X_train, y_train, X_val, y_val, criterion, optimizer, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72152691-c077-441a-9065-50a6906c66fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 13.99227523803711, Validation Loss: 13.004260063171387\n",
      "Epoch 2/20, Training Loss: 13.66125774383545, Validation Loss: 12.688050270080566\n",
      "Epoch 3/20, Training Loss: 13.335793495178223, Validation Loss: 12.377480506896973\n",
      "Epoch 4/20, Training Loss: 13.015976905822754, Validation Loss: 12.072514533996582\n",
      "Epoch 5/20, Training Loss: 12.701765060424805, Validation Loss: 11.772921562194824\n",
      "Epoch 6/20, Training Loss: 12.392902374267578, Validation Loss: 11.478355407714844\n",
      "Epoch 7/20, Training Loss: 12.089025497436523, Validation Loss: 11.188485145568848\n",
      "Epoch 8/20, Training Loss: 11.789800643920898, Validation Loss: 10.903075218200684\n",
      "Epoch 9/20, Training Loss: 11.494999885559082, Validation Loss: 10.621969223022461\n",
      "Epoch 10/20, Training Loss: 11.204482078552246, Validation Loss: 10.345067977905273\n",
      "Epoch 11/20, Training Loss: 10.91816520690918, Validation Loss: 10.072296142578125\n",
      "Epoch 12/20, Training Loss: 10.635987281799316, Validation Loss: 9.803585052490234\n",
      "Epoch 13/20, Training Loss: 10.357889175415039, Validation Loss: 9.538858413696289\n",
      "Epoch 14/20, Training Loss: 10.083806991577148, Validation Loss: 9.278029441833496\n",
      "Epoch 15/20, Training Loss: 9.81365966796875, Validation Loss: 9.021001815795898\n",
      "Epoch 16/20, Training Loss: 9.547357559204102, Validation Loss: 8.767674446105957\n",
      "Epoch 17/20, Training Loss: 9.284804344177246, Validation Loss: 8.51794719696045\n",
      "Epoch 18/20, Training Loss: 9.025901794433594, Validation Loss: 8.271727561950684\n",
      "Epoch 19/20, Training Loss: 8.770556449890137, Validation Loss: 8.028932571411133\n",
      "Epoch 20/20, Training Loss: 8.518685340881348, Validation Loss: 7.789491176605225\n"
     ]
    }
   ],
   "source": [
    "#Modelo con función de activación tanh:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generar datos sintéticos\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 1)\n",
    "y = 3 * X.squeeze() + 2 + np.random.randn(1000) * 0.1\n",
    "\n",
    "# Preparar los datos\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Definir el modelo\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tanh(self.fc1(x))\n",
    "        x = self.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Función para entrenar el modelo\n",
    "def train_model(model, X_train, y_train, X_val, y_val, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "        \n",
    "# Entrenar con tasa de aprendizaje por defecto\n",
    "train_model(model, X_train, y_train, X_val, y_val, criterion, optimizer, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84bc60a-0c6e-4c93-8c78-1795f1ab4245",
   "metadata": {},
   "source": [
    "## Pregunta: ¿Cómo afectan las diferentes funciones de activación en las capas ocultas a la precisión y la pérdida del modelo durante el entrenamiento y la validación?\n",
    "\n",
    "#### Función Sigmoide: Con la función sigmoide, el modelo muestra una disminución constante en la pérdida tanto en el conjunto de entrenamiento como en el de validación a lo largo de las épocas. Sin embargo, la pérdida final sigue siendo relativamente alta, lo que sugiere que el modelo puede no estar capturando de manera efectiva la complejidad de los datos.\n",
    "\n",
    "#### Función Tangente Hiperbólica (tanh): En contraste, con la función tanh, el modelo muestra una disminución más pronunciada en la pérdida, tanto en el conjunto de entrenamiento como en el de validación, a lo largo de las épocas. La pérdida final es considerablemente menor que con la función sigmoide, lo que indica que la función tanh puede estar ayudando al modelo a aprender de manera más efectiva las características de los datos.\n",
    "\n",
    "#### En este caso, la función tanh parece ser más efectiva que la función sigmoide para este problema específico. Es importante probar diferentes funciones de activación y ajustar otros hiperparámetros para encontrar la configuración óptima para el modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
